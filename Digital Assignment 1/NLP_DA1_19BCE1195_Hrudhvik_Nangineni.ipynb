{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-DA1-19BCE1195-Hrudhvik Nangineni.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>CSE4022 Natural Language Processing</center>\n",
        "<center>Digital Assignment -1</center>\n",
        "1.\tUtilize Python NLTK (Natural Language Tool Kit) Platform and do the following. Install relevant Packages and Libraries   <br>                                                                   <p>(7 Marks)</p> <br>\n",
        "<li>Explore Brown Corpus and find the size, tokens, categories,\n",
        "</li>\n",
        "<li>Find the size of word tokens?</li>\n",
        "<li>Find the size of word types?</li>\n",
        "<li>Find the size of category “government”</li>\n",
        "<li>List the most frequent tokens</li>\n",
        "<li>Count the number of sentences</li>\n",
        "<br>\n",
        "2.\tExplore the corpora available in NLTK                                                           <br>                                                                   <p>(3 Marks)</p> <br>\n",
        "\n",
        "Create GitHub account and upload this.\n"
      ],
      "metadata": {
        "id": "4mhrdwLRdyYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hrudhvik Nangineni\n",
        "### 19BCE1195"
      ],
      "metadata": {
        "id": "mdBIhkA58LrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mAwTiG0zdhf3"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdnDbR68pg7v",
        "outputId": "592da32b-1030-4190-9738-956ae320f3fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following. Install relevant Packages and Libraries"
      ],
      "metadata": {
        "id": "9SjzlEbm0jVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Brown Corpus and find the size, tokens, categories,"
      ],
      "metadata": {
        "id": "zQas3aUmqULz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokens"
      ],
      "metadata": {
        "id": "QgFD2N0bsLhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FfRaTLep9eE",
        "outputId": "29a949a8-e647-41d0-8f1b-09a5aca4c0d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size"
      ],
      "metadata": {
        "id": "rVBPDIu9tJot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of brown corpus:\",len(brown.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMhKYp4HsUpK",
        "outputId": "c2e5d998-c901-448f-808c-101d38c8b5ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of brown corpus: 1161192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categories"
      ],
      "metadata": {
        "id": "0wqmx3t7tTvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N4cGfErtRDR",
        "outputId": "49930e78-cfc4-4bbd-e319-13bfea01264c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categories:\",len(brown.categories()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3XFqEP_tYT2",
        "outputId": "c9d9d31a-edc4-4aae-9a6d-39e31cdc8629"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word tokens?"
      ],
      "metadata": {
        "id": "H_MdRgBVtpyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word tokens:\",len(brown.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGtyW53vvvp0",
        "outputId": "7f8ea5d5-f6b1-4ec9-d9ef-3f872d6df0b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word tokens: 1161192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_each_tokens=[]\n",
        "for i in brown.words():\n",
        "  size_of_each_tokens.append([i,len(i)])\n",
        "size_of_each_tokens[:200] # printing forst 200 tokens and there sizes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BVOOLKtjCG",
        "outputId": "83d64b09-2f69-4e84-f371-972a87266c51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 3],\n",
              " ['Fulton', 6],\n",
              " ['County', 6],\n",
              " ['Grand', 5],\n",
              " ['Jury', 4],\n",
              " ['said', 4],\n",
              " ['Friday', 6],\n",
              " ['an', 2],\n",
              " ['investigation', 13],\n",
              " ['of', 2],\n",
              " [\"Atlanta's\", 9],\n",
              " ['recent', 6],\n",
              " ['primary', 7],\n",
              " ['election', 8],\n",
              " ['produced', 8],\n",
              " ['``', 2],\n",
              " ['no', 2],\n",
              " ['evidence', 8],\n",
              " [\"''\", 2],\n",
              " ['that', 4],\n",
              " ['any', 3],\n",
              " ['irregularities', 14],\n",
              " ['took', 4],\n",
              " ['place', 5],\n",
              " ['.', 1],\n",
              " ['The', 3],\n",
              " ['jury', 4],\n",
              " ['further', 7],\n",
              " ['said', 4],\n",
              " ['in', 2],\n",
              " ['term-end', 8],\n",
              " ['presentments', 12],\n",
              " ['that', 4],\n",
              " ['the', 3],\n",
              " ['City', 4],\n",
              " ['Executive', 9],\n",
              " ['Committee', 9],\n",
              " [',', 1],\n",
              " ['which', 5],\n",
              " ['had', 3],\n",
              " ['over-all', 8],\n",
              " ['charge', 6],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['election', 8],\n",
              " [',', 1],\n",
              " ['``', 2],\n",
              " ['deserves', 8],\n",
              " ['the', 3],\n",
              " ['praise', 6],\n",
              " ['and', 3],\n",
              " ['thanks', 6],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['City', 4],\n",
              " ['of', 2],\n",
              " ['Atlanta', 7],\n",
              " [\"''\", 2],\n",
              " ['for', 3],\n",
              " ['the', 3],\n",
              " ['manner', 6],\n",
              " ['in', 2],\n",
              " ['which', 5],\n",
              " ['the', 3],\n",
              " ['election', 8],\n",
              " ['was', 3],\n",
              " ['conducted', 9],\n",
              " ['.', 1],\n",
              " ['The', 3],\n",
              " ['September-October', 17],\n",
              " ['term', 4],\n",
              " ['jury', 4],\n",
              " ['had', 3],\n",
              " ['been', 4],\n",
              " ['charged', 7],\n",
              " ['by', 2],\n",
              " ['Fulton', 6],\n",
              " ['Superior', 8],\n",
              " ['Court', 5],\n",
              " ['Judge', 5],\n",
              " ['Durwood', 7],\n",
              " ['Pye', 3],\n",
              " ['to', 2],\n",
              " ['investigate', 11],\n",
              " ['reports', 7],\n",
              " ['of', 2],\n",
              " ['possible', 8],\n",
              " ['``', 2],\n",
              " ['irregularities', 14],\n",
              " [\"''\", 2],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['hard-fought', 11],\n",
              " ['primary', 7],\n",
              " ['which', 5],\n",
              " ['was', 3],\n",
              " ['won', 3],\n",
              " ['by', 2],\n",
              " ['Mayor-nominate', 14],\n",
              " ['Ivan', 4],\n",
              " ['Allen', 5],\n",
              " ['Jr.', 3],\n",
              " ['.', 1],\n",
              " ['``', 2],\n",
              " ['Only', 4],\n",
              " ['a', 1],\n",
              " ['relative', 8],\n",
              " ['handful', 7],\n",
              " ['of', 2],\n",
              " ['such', 4],\n",
              " ['reports', 7],\n",
              " ['was', 3],\n",
              " ['received', 8],\n",
              " [\"''\", 2],\n",
              " [',', 1],\n",
              " ['the', 3],\n",
              " ['jury', 4],\n",
              " ['said', 4],\n",
              " [',', 1],\n",
              " ['``', 2],\n",
              " ['considering', 11],\n",
              " ['the', 3],\n",
              " ['widespread', 10],\n",
              " ['interest', 8],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['election', 8],\n",
              " [',', 1],\n",
              " ['the', 3],\n",
              " ['number', 6],\n",
              " ['of', 2],\n",
              " ['voters', 6],\n",
              " ['and', 3],\n",
              " ['the', 3],\n",
              " ['size', 4],\n",
              " ['of', 2],\n",
              " ['this', 4],\n",
              " ['city', 4],\n",
              " [\"''\", 2],\n",
              " ['.', 1],\n",
              " ['The', 3],\n",
              " ['jury', 4],\n",
              " ['said', 4],\n",
              " ['it', 2],\n",
              " ['did', 3],\n",
              " ['find', 4],\n",
              " ['that', 4],\n",
              " ['many', 4],\n",
              " ['of', 2],\n",
              " [\"Georgia's\", 9],\n",
              " ['registration', 12],\n",
              " ['and', 3],\n",
              " ['election', 8],\n",
              " ['laws', 4],\n",
              " ['``', 2],\n",
              " ['are', 3],\n",
              " ['outmoded', 8],\n",
              " ['or', 2],\n",
              " ['inadequate', 10],\n",
              " ['and', 3],\n",
              " ['often', 5],\n",
              " ['ambiguous', 9],\n",
              " [\"''\", 2],\n",
              " ['.', 1],\n",
              " ['It', 2],\n",
              " ['recommended', 11],\n",
              " ['that', 4],\n",
              " ['Fulton', 6],\n",
              " ['legislators', 11],\n",
              " ['act', 3],\n",
              " ['``', 2],\n",
              " ['to', 2],\n",
              " ['have', 4],\n",
              " ['these', 5],\n",
              " ['laws', 4],\n",
              " ['studied', 7],\n",
              " ['and', 3],\n",
              " ['revised', 7],\n",
              " ['to', 2],\n",
              " ['the', 3],\n",
              " ['end', 3],\n",
              " ['of', 2],\n",
              " ['modernizing', 11],\n",
              " ['and', 3],\n",
              " ['improving', 9],\n",
              " ['them', 4],\n",
              " [\"''\", 2],\n",
              " ['.', 1],\n",
              " ['The', 3],\n",
              " ['grand', 5],\n",
              " ['jury', 4],\n",
              " ['commented', 9],\n",
              " ['on', 2],\n",
              " ['a', 1],\n",
              " ['number', 6],\n",
              " ['of', 2],\n",
              " ['other', 5],\n",
              " ['topics', 6],\n",
              " [',', 1],\n",
              " ['among', 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word types?"
      ],
      "metadata": {
        "id": "dSwSsVlPukqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word types:\",len(set(brown.words())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivX2qsfmuFCm",
        "outputId": "9524431c-5db2-4b3f-f190-bd3bc0b98bca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of category “government”"
      ],
      "metadata": {
        "id": "G90LLmtPwGvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gov_words=brown.words(categories='government')\n",
        "gov_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eozAZ3UywIgp",
        "outputId": "d59aee20-3058-45ba-972b-7128115c0ab6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Office', 'of', 'Business', 'Economics', '(', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word types:\",len(gov_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL9x_Kq8wc1l",
        "outputId": "cbb1a5fa-1403-40c6-cfd7-586ee8321b0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types: 70117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List the most frequent tokens"
      ],
      "metadata": {
        "id": "q7taKA8Ewtcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=brown.words()\n",
        "words_count=nltk.FreqDist(w.lower() for w in words)\n",
        "frequent_words_count=dict(sorted(words_count.items(), key=lambda item: item[1],reverse=True))"
      ],
      "metadata": {
        "id": "_OpbL8wKwrUN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 200 frequent tokens\\n\")\n",
        "keys=list(frequent_words_count.keys())\n",
        "for i in range(200):\n",
        "  print(keys[i],frequent_words_count[keys[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nndXgcw_p0",
        "outputId": "a686b38b-6913-4db5-c705-0ba6dcbcdbab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 200 frequent tokens\n",
            "\n",
            "the 69971\n",
            ", 58334\n",
            ". 49346\n",
            "of 36412\n",
            "and 28853\n",
            "to 26158\n",
            "a 23195\n",
            "in 21337\n",
            "that 10594\n",
            "is 10109\n",
            "was 9815\n",
            "he 9548\n",
            "for 9489\n",
            "`` 8837\n",
            "'' 8789\n",
            "it 8760\n",
            "with 7289\n",
            "as 7253\n",
            "his 6996\n",
            "on 6741\n",
            "be 6377\n",
            "; 5566\n",
            "at 5372\n",
            "by 5306\n",
            "i 5164\n",
            "this 5145\n",
            "had 5133\n",
            "? 4693\n",
            "not 4610\n",
            "are 4394\n",
            "but 4381\n",
            "from 4370\n",
            "or 4206\n",
            "have 3942\n",
            "an 3740\n",
            "they 3620\n",
            "which 3561\n",
            "-- 3432\n",
            "one 3292\n",
            "you 3286\n",
            "were 3284\n",
            "her 3036\n",
            "all 3001\n",
            "she 2860\n",
            "there 2728\n",
            "would 2714\n",
            "their 2669\n",
            "we 2652\n",
            "him 2619\n",
            "been 2472\n",
            ") 2466\n",
            "has 2437\n",
            "( 2435\n",
            "when 2331\n",
            "who 2252\n",
            "will 2245\n",
            "more 2215\n",
            "if 2198\n",
            "no 2139\n",
            "out 2097\n",
            "so 1985\n",
            "said 1961\n",
            "what 1908\n",
            "up 1890\n",
            "its 1858\n",
            "about 1815\n",
            ": 1795\n",
            "into 1791\n",
            "than 1790\n",
            "them 1788\n",
            "can 1772\n",
            "only 1748\n",
            "other 1702\n",
            "new 1635\n",
            "some 1618\n",
            "could 1601\n",
            "time 1598\n",
            "! 1596\n",
            "these 1573\n",
            "two 1412\n",
            "may 1402\n",
            "then 1380\n",
            "do 1363\n",
            "first 1361\n",
            "any 1344\n",
            "my 1318\n",
            "now 1314\n",
            "such 1303\n",
            "like 1292\n",
            "our 1252\n",
            "over 1236\n",
            "man 1207\n",
            "me 1181\n",
            "even 1170\n",
            "most 1159\n",
            "made 1125\n",
            "also 1069\n",
            "after 1069\n",
            "did 1044\n",
            "many 1030\n",
            "before 1016\n",
            "must 1013\n",
            "af 996\n",
            "through 971\n",
            "back 966\n",
            "years 950\n",
            "where 937\n",
            "much 937\n",
            "your 923\n",
            "way 908\n",
            "well 897\n",
            "down 895\n",
            "should 888\n",
            "because 883\n",
            "each 877\n",
            "just 872\n",
            "those 850\n",
            "people 847\n",
            "mr. 844\n",
            "too 834\n",
            "how 834\n",
            "little 831\n",
            "state 807\n",
            "good 806\n",
            "very 796\n",
            "make 794\n",
            "world 787\n",
            "still 782\n",
            "see 772\n",
            "own 772\n",
            "men 763\n",
            "work 762\n",
            "long 752\n",
            "here 750\n",
            "get 749\n",
            "both 730\n",
            "between 730\n",
            "life 715\n",
            "being 712\n",
            "under 707\n",
            "never 697\n",
            "day 687\n",
            "same 686\n",
            "another 684\n",
            "know 683\n",
            "while 680\n",
            "last 676\n",
            "us 675\n",
            "might 672\n",
            "great 665\n",
            "old 661\n",
            "year 658\n",
            "off 639\n",
            "come 630\n",
            "since 628\n",
            "against 627\n",
            "go 626\n",
            "came 622\n",
            "right 613\n",
            "used 611\n",
            "take 610\n",
            "three 610\n",
            "himself 603\n",
            "states 603\n",
            "few 601\n",
            "house 591\n",
            "use 591\n",
            "during 585\n",
            "without 583\n",
            "again 577\n",
            "place 570\n",
            "american 569\n",
            "around 562\n",
            "however 552\n",
            "home 547\n",
            "small 542\n",
            "found 536\n",
            "mrs. 534\n",
            "1 527\n",
            "thought 517\n",
            "went 507\n",
            "say 504\n",
            "part 500\n",
            "once 499\n",
            "general 498\n",
            "high 497\n",
            "upon 495\n",
            "school 493\n",
            "every 491\n",
            "don't 489\n",
            "does 485\n",
            "got 482\n",
            "united 482\n",
            "left 480\n",
            "number 472\n",
            "course 465\n",
            "war 464\n",
            "until 461\n",
            "always 458\n",
            "away 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count the number of sentences"
      ],
      "metadata": {
        "id": "2n8uPjt2z8tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlGc7u17zNBx",
        "outputId": "17f4d2c9-3e79-4b2c-ab90-e0d6a419c2c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here each sentence is a sub array"
      ],
      "metadata": {
        "id": "gFrtFzQt0ODY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sentences:\",len(brown.sents()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw2Zx-Sa0NKk",
        "outputId": "c86b51b2-a10f-477f-b60e-64897d92a2d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 57340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explore the corpora available in NLTK"
      ],
      "metadata": {
        "id": "kJWI2rXA0dp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reuters Corpus"
      ],
      "metadata": {
        "id": "ktXIb-Y38uxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNe95g_50B-E",
        "outputId": "017e8fef-3a00-4b56-b043-eb50663a6060"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Reuters Corpus and find the size, tokens, categories,"
      ],
      "metadata": {
        "id": "91DFtGC185Wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokens"
      ],
      "metadata": {
        "id": "o1bCflAZ85Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a95b826-154c-404e-aa24-d4fa36958212",
        "id": "slrddoEs85Wn"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size"
      ],
      "metadata": {
        "id": "ck494rzU85Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of reuters corpus:\",len(reuters.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3091114-a5c7-42dc-e898-a06c4ae1e3d3",
        "id": "5nz5v_Jl85Wq"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of reuters corpus: 1720901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categories"
      ],
      "metadata": {
        "id": "RVUzjbbE85Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6150d102-8e95-42e9-dbae-3cd5b7e4118e",
        "id": "5951ZaW385Wu"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['acq',\n",
              " 'alum',\n",
              " 'barley',\n",
              " 'bop',\n",
              " 'carcass',\n",
              " 'castor-oil',\n",
              " 'cocoa',\n",
              " 'coconut',\n",
              " 'coconut-oil',\n",
              " 'coffee',\n",
              " 'copper',\n",
              " 'copra-cake',\n",
              " 'corn',\n",
              " 'cotton',\n",
              " 'cotton-oil',\n",
              " 'cpi',\n",
              " 'cpu',\n",
              " 'crude',\n",
              " 'dfl',\n",
              " 'dlr',\n",
              " 'dmk',\n",
              " 'earn',\n",
              " 'fuel',\n",
              " 'gas',\n",
              " 'gnp',\n",
              " 'gold',\n",
              " 'grain',\n",
              " 'groundnut',\n",
              " 'groundnut-oil',\n",
              " 'heat',\n",
              " 'hog',\n",
              " 'housing',\n",
              " 'income',\n",
              " 'instal-debt',\n",
              " 'interest',\n",
              " 'ipi',\n",
              " 'iron-steel',\n",
              " 'jet',\n",
              " 'jobs',\n",
              " 'l-cattle',\n",
              " 'lead',\n",
              " 'lei',\n",
              " 'lin-oil',\n",
              " 'livestock',\n",
              " 'lumber',\n",
              " 'meal-feed',\n",
              " 'money-fx',\n",
              " 'money-supply',\n",
              " 'naphtha',\n",
              " 'nat-gas',\n",
              " 'nickel',\n",
              " 'nkr',\n",
              " 'nzdlr',\n",
              " 'oat',\n",
              " 'oilseed',\n",
              " 'orange',\n",
              " 'palladium',\n",
              " 'palm-oil',\n",
              " 'palmkernel',\n",
              " 'pet-chem',\n",
              " 'platinum',\n",
              " 'potato',\n",
              " 'propane',\n",
              " 'rand',\n",
              " 'rape-oil',\n",
              " 'rapeseed',\n",
              " 'reserves',\n",
              " 'retail',\n",
              " 'rice',\n",
              " 'rubber',\n",
              " 'rye',\n",
              " 'ship',\n",
              " 'silver',\n",
              " 'sorghum',\n",
              " 'soy-meal',\n",
              " 'soy-oil',\n",
              " 'soybean',\n",
              " 'strategic-metal',\n",
              " 'sugar',\n",
              " 'sun-meal',\n",
              " 'sun-oil',\n",
              " 'sunseed',\n",
              " 'tea',\n",
              " 'tin',\n",
              " 'trade',\n",
              " 'veg-oil',\n",
              " 'wheat',\n",
              " 'wpi',\n",
              " 'yen',\n",
              " 'zinc']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categories:\",len(reuters.categories()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc6aa16-d4b8-46ad-b27a-6557a19e4778",
        "id": "jCAlTfet85Wv"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word tokens in Reuters Corpus?"
      ],
      "metadata": {
        "id": "1__9NZfm85Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word tokens in reuters corpus:\",len(reuters.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72153715-ee04-4348-fd94-1e198aef3b3c",
        "id": "6r0WkVJf85Wx"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word tokens in reuters corpus: 1720901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_each_tokens_in_reuters=[]\n",
        "for i in reuters.words():\n",
        "  size_of_each_tokens_in_reuters.append([i,len(i)])\n",
        "size_of_each_tokens_in_reuters[:200] # printing forst 200 tokens and there sizes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854bc51a-4b75-4337-9a47-17ef3100759d",
        "id": "PN1gyCjl85Wy"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ASIAN', 5],\n",
              " ['EXPORTERS', 9],\n",
              " ['FEAR', 4],\n",
              " ['DAMAGE', 6],\n",
              " ['FROM', 4],\n",
              " ['U', 1],\n",
              " ['.', 1],\n",
              " ['S', 1],\n",
              " ['.-', 2],\n",
              " ['JAPAN', 5],\n",
              " ['RIFT', 4],\n",
              " ['Mounting', 8],\n",
              " ['trade', 5],\n",
              " ['friction', 8],\n",
              " ['between', 7],\n",
              " ['the', 3],\n",
              " ['U', 1],\n",
              " ['.', 1],\n",
              " ['S', 1],\n",
              " ['.', 1],\n",
              " ['And', 3],\n",
              " ['Japan', 5],\n",
              " ['has', 3],\n",
              " ['raised', 6],\n",
              " ['fears', 5],\n",
              " ['among', 5],\n",
              " ['many', 4],\n",
              " ['of', 2],\n",
              " ['Asia', 4],\n",
              " [\"'\", 1],\n",
              " ['s', 1],\n",
              " ['exporting', 9],\n",
              " ['nations', 7],\n",
              " ['that', 4],\n",
              " ['the', 3],\n",
              " ['row', 3],\n",
              " ['could', 5],\n",
              " ['inflict', 7],\n",
              " ['far', 3],\n",
              " ['-', 1],\n",
              " ['reaching', 8],\n",
              " ['economic', 8],\n",
              " ['damage', 6],\n",
              " [',', 1],\n",
              " ['businessmen', 11],\n",
              " ['and', 3],\n",
              " ['officials', 9],\n",
              " ['said', 4],\n",
              " ['.', 1],\n",
              " ['They', 4],\n",
              " ['told', 4],\n",
              " ['Reuter', 6],\n",
              " ['correspondents', 14],\n",
              " ['in', 2],\n",
              " ['Asian', 5],\n",
              " ['capitals', 8],\n",
              " ['a', 1],\n",
              " ['U', 1],\n",
              " ['.', 1],\n",
              " ['S', 1],\n",
              " ['.', 1],\n",
              " ['Move', 4],\n",
              " ['against', 7],\n",
              " ['Japan', 5],\n",
              " ['might', 5],\n",
              " ['boost', 5],\n",
              " ['protectionist', 13],\n",
              " ['sentiment', 9],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['U', 1],\n",
              " ['.', 1],\n",
              " ['S', 1],\n",
              " ['.', 1],\n",
              " ['And', 3],\n",
              " ['lead', 4],\n",
              " ['to', 2],\n",
              " ['curbs', 5],\n",
              " ['on', 2],\n",
              " ['American', 8],\n",
              " ['imports', 7],\n",
              " ['of', 2],\n",
              " ['their', 5],\n",
              " ['products', 8],\n",
              " ['.', 1],\n",
              " ['But', 3],\n",
              " ['some', 4],\n",
              " ['exporters', 9],\n",
              " ['said', 4],\n",
              " ['that', 4],\n",
              " ['while', 5],\n",
              " ['the', 3],\n",
              " ['conflict', 8],\n",
              " ['would', 5],\n",
              " ['hurt', 4],\n",
              " ['them', 4],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['long', 4],\n",
              " ['-', 1],\n",
              " ['run', 3],\n",
              " [',', 1],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['short', 5],\n",
              " ['-', 1],\n",
              " ['term', 4],\n",
              " ['Tokyo', 5],\n",
              " [\"'\", 1],\n",
              " ['s', 1],\n",
              " ['loss', 4],\n",
              " ['might', 5],\n",
              " ['be', 2],\n",
              " ['their', 5],\n",
              " ['gain', 4],\n",
              " ['.', 1],\n",
              " ['The', 3],\n",
              " ['U', 1],\n",
              " ['.', 1],\n",
              " ['S', 1],\n",
              " ['.', 1],\n",
              " ['Has', 3],\n",
              " ['said', 4],\n",
              " ['it', 2],\n",
              " ['will', 4],\n",
              " ['impose', 6],\n",
              " ['300', 3],\n",
              " ['mln', 3],\n",
              " ['dlrs', 4],\n",
              " ['of', 2],\n",
              " ['tariffs', 7],\n",
              " ['on', 2],\n",
              " ['imports', 7],\n",
              " ['of', 2],\n",
              " ['Japanese', 8],\n",
              " ['electronics', 11],\n",
              " ['goods', 5],\n",
              " ['on', 2],\n",
              " ['April', 5],\n",
              " ['17', 2],\n",
              " [',', 1],\n",
              " ['in', 2],\n",
              " ['retaliation', 11],\n",
              " ['for', 3],\n",
              " ['Japan', 5],\n",
              " [\"'\", 1],\n",
              " ['s', 1],\n",
              " ['alleged', 7],\n",
              " ['failure', 7],\n",
              " ['to', 2],\n",
              " ['stick', 5],\n",
              " ['to', 2],\n",
              " ['a', 1],\n",
              " ['pact', 4],\n",
              " ['not', 3],\n",
              " ['to', 2],\n",
              " ['sell', 4],\n",
              " ['semiconductors', 14],\n",
              " ['on', 2],\n",
              " ['world', 5],\n",
              " ['markets', 7],\n",
              " ['at', 2],\n",
              " ['below', 5],\n",
              " ['cost', 4],\n",
              " ['.', 1],\n",
              " ['Unofficial', 10],\n",
              " ['Japanese', 8],\n",
              " ['estimates', 9],\n",
              " ['put', 3],\n",
              " ['the', 3],\n",
              " ['impact', 6],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['tariffs', 7],\n",
              " ['at', 2],\n",
              " ['10', 2],\n",
              " ['billion', 7],\n",
              " ['dlrs', 4],\n",
              " ['and', 3],\n",
              " ['spokesmen', 9],\n",
              " ['for', 3],\n",
              " ['major', 5],\n",
              " ['electronics', 11],\n",
              " ['firms', 5],\n",
              " ['said', 4],\n",
              " ['they', 4],\n",
              " ['would', 5],\n",
              " ['virtually', 9],\n",
              " ['halt', 4],\n",
              " ['exports', 7],\n",
              " ['of', 2],\n",
              " ['products', 8],\n",
              " ['hit', 3],\n",
              " ['by', 2],\n",
              " ['the', 3],\n",
              " ['new', 3],\n",
              " ['taxes', 5],\n",
              " ['.', 1],\n",
              " ['\"', 1],\n",
              " ['We', 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word types in Reuters Corpus?"
      ],
      "metadata": {
        "id": "2IiHrFEZ85Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word types in reuters corpus:\",len(set(reuters.words())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a88a66-372a-48ca-feea-9706a21a6f17",
        "id": "QdBKCJ6g85W0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types in reuters corpus: 41600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List the most frequent tokens in Reuters Corpus"
      ],
      "metadata": {
        "id": "8EX_zyFC85W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_reuters=reuters.words()\n",
        "words_count_reuters=nltk.FreqDist(w.lower() for w in words_reuters)\n",
        "frequent_words_count_reuters=dict(sorted(words_count.items(), key=lambda item: item[1],reverse=True))"
      ],
      "metadata": {
        "id": "YjjDvWSo85W1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 200 frequent tokens in reuters corpus\\n\")\n",
        "keys_reuters=list(frequent_words_count_reuters.keys())\n",
        "for i in range(200):\n",
        "  print(keys_reuters[i],frequent_words_count_reuters[keys_reuters[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bd15e3-d600-44cc-f859-5cfd778d7244",
        "id": "u73Nk8qN85W2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 200 frequent tokens in reuters corpus\n",
            "\n",
            "the 69971\n",
            ", 58334\n",
            ". 49346\n",
            "of 36412\n",
            "and 28853\n",
            "to 26158\n",
            "a 23195\n",
            "in 21337\n",
            "that 10594\n",
            "is 10109\n",
            "was 9815\n",
            "he 9548\n",
            "for 9489\n",
            "`` 8837\n",
            "'' 8789\n",
            "it 8760\n",
            "with 7289\n",
            "as 7253\n",
            "his 6996\n",
            "on 6741\n",
            "be 6377\n",
            "; 5566\n",
            "at 5372\n",
            "by 5306\n",
            "i 5164\n",
            "this 5145\n",
            "had 5133\n",
            "? 4693\n",
            "not 4610\n",
            "are 4394\n",
            "but 4381\n",
            "from 4370\n",
            "or 4206\n",
            "have 3942\n",
            "an 3740\n",
            "they 3620\n",
            "which 3561\n",
            "-- 3432\n",
            "one 3292\n",
            "you 3286\n",
            "were 3284\n",
            "her 3036\n",
            "all 3001\n",
            "she 2860\n",
            "there 2728\n",
            "would 2714\n",
            "their 2669\n",
            "we 2652\n",
            "him 2619\n",
            "been 2472\n",
            ") 2466\n",
            "has 2437\n",
            "( 2435\n",
            "when 2331\n",
            "who 2252\n",
            "will 2245\n",
            "more 2215\n",
            "if 2198\n",
            "no 2139\n",
            "out 2097\n",
            "so 1985\n",
            "said 1961\n",
            "what 1908\n",
            "up 1890\n",
            "its 1858\n",
            "about 1815\n",
            ": 1795\n",
            "into 1791\n",
            "than 1790\n",
            "them 1788\n",
            "can 1772\n",
            "only 1748\n",
            "other 1702\n",
            "new 1635\n",
            "some 1618\n",
            "could 1601\n",
            "time 1598\n",
            "! 1596\n",
            "these 1573\n",
            "two 1412\n",
            "may 1402\n",
            "then 1380\n",
            "do 1363\n",
            "first 1361\n",
            "any 1344\n",
            "my 1318\n",
            "now 1314\n",
            "such 1303\n",
            "like 1292\n",
            "our 1252\n",
            "over 1236\n",
            "man 1207\n",
            "me 1181\n",
            "even 1170\n",
            "most 1159\n",
            "made 1125\n",
            "also 1069\n",
            "after 1069\n",
            "did 1044\n",
            "many 1030\n",
            "before 1016\n",
            "must 1013\n",
            "af 996\n",
            "through 971\n",
            "back 966\n",
            "years 950\n",
            "where 937\n",
            "much 937\n",
            "your 923\n",
            "way 908\n",
            "well 897\n",
            "down 895\n",
            "should 888\n",
            "because 883\n",
            "each 877\n",
            "just 872\n",
            "those 850\n",
            "people 847\n",
            "mr. 844\n",
            "too 834\n",
            "how 834\n",
            "little 831\n",
            "state 807\n",
            "good 806\n",
            "very 796\n",
            "make 794\n",
            "world 787\n",
            "still 782\n",
            "see 772\n",
            "own 772\n",
            "men 763\n",
            "work 762\n",
            "long 752\n",
            "here 750\n",
            "get 749\n",
            "both 730\n",
            "between 730\n",
            "life 715\n",
            "being 712\n",
            "under 707\n",
            "never 697\n",
            "day 687\n",
            "same 686\n",
            "another 684\n",
            "know 683\n",
            "while 680\n",
            "last 676\n",
            "us 675\n",
            "might 672\n",
            "great 665\n",
            "old 661\n",
            "year 658\n",
            "off 639\n",
            "come 630\n",
            "since 628\n",
            "against 627\n",
            "go 626\n",
            "came 622\n",
            "right 613\n",
            "used 611\n",
            "take 610\n",
            "three 610\n",
            "himself 603\n",
            "states 603\n",
            "few 601\n",
            "house 591\n",
            "use 591\n",
            "during 585\n",
            "without 583\n",
            "again 577\n",
            "place 570\n",
            "american 569\n",
            "around 562\n",
            "however 552\n",
            "home 547\n",
            "small 542\n",
            "found 536\n",
            "mrs. 534\n",
            "1 527\n",
            "thought 517\n",
            "went 507\n",
            "say 504\n",
            "part 500\n",
            "once 499\n",
            "general 498\n",
            "high 497\n",
            "upon 495\n",
            "school 493\n",
            "every 491\n",
            "don't 489\n",
            "does 485\n",
            "got 482\n",
            "united 482\n",
            "left 480\n",
            "number 472\n",
            "course 465\n",
            "war 464\n",
            "until 461\n",
            "always 458\n",
            "away 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count the number of sentences in Reuters Corpus"
      ],
      "metadata": {
        "id": "_s4vmU5e85W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa2a59c-f104-4fc7-e5b0-98cbbe71c4bd",
        "id": "SDLFKFn185W3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here each sentence is a sub array"
      ],
      "metadata": {
        "id": "QJiqpV1V85W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sentences in reuters corpus:\",len(reuters.sents()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c337ee49-3be2-47ce-8031-827de04f2cf4",
        "id": "eVOpdXkj85W4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in reuters corpus: 54711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BQSH_E9Q-FC0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inaugural Address Corpus"
      ],
      "metadata": {
        "id": "Hcrzx33R-oGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import inaugural\n",
        "nltk.download('inaugural')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ca3535-61dc-4778-f11d-023b57f51368",
        "id": "DhF7FQID-oHG"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Inaugural Address Corpus and find the size, tokens"
      ],
      "metadata": {
        "id": "UJcu3kca-oHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokens"
      ],
      "metadata": {
        "id": "KsOHibh0-oHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inaugural.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43ec759-9f20-4bf9-e241-80717a91a9ce",
        "id": "s3M1f4ve-oHJ"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size"
      ],
      "metadata": {
        "id": "LSdc7kE9-oHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of inaugural corpus:\",len(inaugural.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a389f3b-be07-4085-8ae5-119b16eaf5a1",
        "id": "INvhIWsB-oHK"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of inaugural corpus: 152901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word tokens in Inaugural Address Corpus?"
      ],
      "metadata": {
        "id": "VEIZ2Imr-oHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word tokens in inaugural corpus:\",len(inaugural.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a07369-21a6-415b-fec5-52fa45c45854",
        "id": "wTtkhaGJ-oHM"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word tokens in inaugural corpus: 152901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_each_tokens_in_inaugural=[]\n",
        "for i in inaugural.words():\n",
        "  size_of_each_tokens_in_inaugural.append([i,len(i)])\n",
        "size_of_each_tokens_in_inaugural[:200] # printing forst 200 tokens and there sizes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1092e5e8-f352-4a06-da62-f4cd4dd4ef25",
        "id": "kHZqbHv4-oHN"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Fellow', 6],\n",
              " ['-', 1],\n",
              " ['Citizens', 8],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['Senate', 6],\n",
              " ['and', 3],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['House', 5],\n",
              " ['of', 2],\n",
              " ['Representatives', 15],\n",
              " [':', 1],\n",
              " ['Among', 5],\n",
              " ['the', 3],\n",
              " ['vicissitudes', 12],\n",
              " ['incident', 8],\n",
              " ['to', 2],\n",
              " ['life', 4],\n",
              " ['no', 2],\n",
              " ['event', 5],\n",
              " ['could', 5],\n",
              " ['have', 4],\n",
              " ['filled', 6],\n",
              " ['me', 2],\n",
              " ['with', 4],\n",
              " ['greater', 7],\n",
              " ['anxieties', 9],\n",
              " ['than', 4],\n",
              " ['that', 4],\n",
              " ['of', 2],\n",
              " ['which', 5],\n",
              " ['the', 3],\n",
              " ['notification', 12],\n",
              " ['was', 3],\n",
              " ['transmitted', 11],\n",
              " ['by', 2],\n",
              " ['your', 4],\n",
              " ['order', 5],\n",
              " [',', 1],\n",
              " ['and', 3],\n",
              " ['received', 8],\n",
              " ['on', 2],\n",
              " ['the', 3],\n",
              " ['14th', 4],\n",
              " ['day', 3],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['present', 7],\n",
              " ['month', 5],\n",
              " ['.', 1],\n",
              " ['On', 2],\n",
              " ['the', 3],\n",
              " ['one', 3],\n",
              " ['hand', 4],\n",
              " [',', 1],\n",
              " ['I', 1],\n",
              " ['was', 3],\n",
              " ['summoned', 8],\n",
              " ['by', 2],\n",
              " ['my', 2],\n",
              " ['Country', 7],\n",
              " [',', 1],\n",
              " ['whose', 5],\n",
              " ['voice', 5],\n",
              " ['I', 1],\n",
              " ['can', 3],\n",
              " ['never', 5],\n",
              " ['hear', 4],\n",
              " ['but', 3],\n",
              " ['with', 4],\n",
              " ['veneration', 10],\n",
              " ['and', 3],\n",
              " ['love', 4],\n",
              " [',', 1],\n",
              " ['from', 4],\n",
              " ['a', 1],\n",
              " ['retreat', 7],\n",
              " ['which', 5],\n",
              " ['I', 1],\n",
              " ['had', 3],\n",
              " ['chosen', 6],\n",
              " ['with', 4],\n",
              " ['the', 3],\n",
              " ['fondest', 7],\n",
              " ['predilection', 12],\n",
              " [',', 1],\n",
              " ['and', 3],\n",
              " [',', 1],\n",
              " ['in', 2],\n",
              " ['my', 2],\n",
              " ['flattering', 10],\n",
              " ['hopes', 5],\n",
              " [',', 1],\n",
              " ['with', 4],\n",
              " ['an', 2],\n",
              " ['immutable', 9],\n",
              " ['decision', 8],\n",
              " [',', 1],\n",
              " ['as', 2],\n",
              " ['the', 3],\n",
              " ['asylum', 6],\n",
              " ['of', 2],\n",
              " ['my', 2],\n",
              " ['declining', 9],\n",
              " ['years', 5],\n",
              " ['--', 2],\n",
              " ['a', 1],\n",
              " ['retreat', 7],\n",
              " ['which', 5],\n",
              " ['was', 3],\n",
              " ['rendered', 8],\n",
              " ['every', 5],\n",
              " ['day', 3],\n",
              " ['more', 4],\n",
              " ['necessary', 9],\n",
              " ['as', 2],\n",
              " ['well', 4],\n",
              " ['as', 2],\n",
              " ['more', 4],\n",
              " ['dear', 4],\n",
              " ['to', 2],\n",
              " ['me', 2],\n",
              " ['by', 2],\n",
              " ['the', 3],\n",
              " ['addition', 8],\n",
              " ['of', 2],\n",
              " ['habit', 5],\n",
              " ['to', 2],\n",
              " ['inclination', 11],\n",
              " [',', 1],\n",
              " ['and', 3],\n",
              " ['of', 2],\n",
              " ['frequent', 8],\n",
              " ['interruptions', 13],\n",
              " ['in', 2],\n",
              " ['my', 2],\n",
              " ['health', 6],\n",
              " ['to', 2],\n",
              " ['the', 3],\n",
              " ['gradual', 7],\n",
              " ['waste', 5],\n",
              " ['committed', 9],\n",
              " ['on', 2],\n",
              " ['it', 2],\n",
              " ['by', 2],\n",
              " ['time', 4],\n",
              " ['.', 1],\n",
              " ['On', 2],\n",
              " ['the', 3],\n",
              " ['other', 5],\n",
              " ['hand', 4],\n",
              " [',', 1],\n",
              " ['the', 3],\n",
              " ['magnitude', 9],\n",
              " ['and', 3],\n",
              " ['difficulty', 10],\n",
              " ['of', 2],\n",
              " ['the', 3],\n",
              " ['trust', 5],\n",
              " ['to', 2],\n",
              " ['which', 5],\n",
              " ['the', 3],\n",
              " ['voice', 5],\n",
              " ['of', 2],\n",
              " ['my', 2],\n",
              " ['country', 7],\n",
              " ['called', 6],\n",
              " ['me', 2],\n",
              " [',', 1],\n",
              " ['being', 5],\n",
              " ['sufficient', 10],\n",
              " ['to', 2],\n",
              " ['awaken', 6],\n",
              " ['in', 2],\n",
              " ['the', 3],\n",
              " ['wisest', 6],\n",
              " ['and', 3],\n",
              " ['most', 4],\n",
              " ['experienced', 11],\n",
              " ['of', 2],\n",
              " ['her', 3],\n",
              " ['citizens', 8],\n",
              " ['a', 1],\n",
              " ['distrustful', 11],\n",
              " ['scrutiny', 8],\n",
              " ['into', 4],\n",
              " ['his', 3],\n",
              " ['qualifications', 14],\n",
              " [',', 1],\n",
              " ['could', 5],\n",
              " ['not', 3],\n",
              " ['but', 3],\n",
              " ['overwhelm', 9],\n",
              " ['with', 4],\n",
              " ['despondence', 11],\n",
              " ['one', 3],\n",
              " ['who', 3],\n",
              " ['(', 1],\n",
              " ['inheriting', 10]]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the size of word types in Inaugural Address Corpus?"
      ],
      "metadata": {
        "id": "QIwmjhh8-oHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of word types in inaugural corpus:\",len(set(inaugural.words())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be6be1c-1068-4b1c-d175-901f2d4cff71",
        "id": "po6jEw4s-oHO"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types in inaugural corpus: 10025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List the most frequent tokens in Inaugural Address Corpus"
      ],
      "metadata": {
        "id": "_B-NLB-j-oHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_inaugural=inaugural.words()\n",
        "words_count_inaugural=nltk.FreqDist(w.lower() for w in words_inaugural)\n",
        "frequent_words_count_inaugural=dict(sorted(words_count.items(), key=lambda item: item[1],reverse=True))"
      ],
      "metadata": {
        "id": "TVNhLQAb-oHP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 200 frequent tokens in inaugural corpus\\n\")\n",
        "keys_inaugural=list(frequent_words_count_inaugural.keys())\n",
        "for i in range(200):\n",
        "  print(keys_inaugural[i],frequent_words_count_inaugural[keys_inaugural[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd6ae4a-e5a2-4210-d030-a05bca7cbc2a",
        "id": "G8ElCyqN-oHP"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 200 frequent tokens in inaugural corpus\n",
            "\n",
            "the 69971\n",
            ", 58334\n",
            ". 49346\n",
            "of 36412\n",
            "and 28853\n",
            "to 26158\n",
            "a 23195\n",
            "in 21337\n",
            "that 10594\n",
            "is 10109\n",
            "was 9815\n",
            "he 9548\n",
            "for 9489\n",
            "`` 8837\n",
            "'' 8789\n",
            "it 8760\n",
            "with 7289\n",
            "as 7253\n",
            "his 6996\n",
            "on 6741\n",
            "be 6377\n",
            "; 5566\n",
            "at 5372\n",
            "by 5306\n",
            "i 5164\n",
            "this 5145\n",
            "had 5133\n",
            "? 4693\n",
            "not 4610\n",
            "are 4394\n",
            "but 4381\n",
            "from 4370\n",
            "or 4206\n",
            "have 3942\n",
            "an 3740\n",
            "they 3620\n",
            "which 3561\n",
            "-- 3432\n",
            "one 3292\n",
            "you 3286\n",
            "were 3284\n",
            "her 3036\n",
            "all 3001\n",
            "she 2860\n",
            "there 2728\n",
            "would 2714\n",
            "their 2669\n",
            "we 2652\n",
            "him 2619\n",
            "been 2472\n",
            ") 2466\n",
            "has 2437\n",
            "( 2435\n",
            "when 2331\n",
            "who 2252\n",
            "will 2245\n",
            "more 2215\n",
            "if 2198\n",
            "no 2139\n",
            "out 2097\n",
            "so 1985\n",
            "said 1961\n",
            "what 1908\n",
            "up 1890\n",
            "its 1858\n",
            "about 1815\n",
            ": 1795\n",
            "into 1791\n",
            "than 1790\n",
            "them 1788\n",
            "can 1772\n",
            "only 1748\n",
            "other 1702\n",
            "new 1635\n",
            "some 1618\n",
            "could 1601\n",
            "time 1598\n",
            "! 1596\n",
            "these 1573\n",
            "two 1412\n",
            "may 1402\n",
            "then 1380\n",
            "do 1363\n",
            "first 1361\n",
            "any 1344\n",
            "my 1318\n",
            "now 1314\n",
            "such 1303\n",
            "like 1292\n",
            "our 1252\n",
            "over 1236\n",
            "man 1207\n",
            "me 1181\n",
            "even 1170\n",
            "most 1159\n",
            "made 1125\n",
            "also 1069\n",
            "after 1069\n",
            "did 1044\n",
            "many 1030\n",
            "before 1016\n",
            "must 1013\n",
            "af 996\n",
            "through 971\n",
            "back 966\n",
            "years 950\n",
            "where 937\n",
            "much 937\n",
            "your 923\n",
            "way 908\n",
            "well 897\n",
            "down 895\n",
            "should 888\n",
            "because 883\n",
            "each 877\n",
            "just 872\n",
            "those 850\n",
            "people 847\n",
            "mr. 844\n",
            "too 834\n",
            "how 834\n",
            "little 831\n",
            "state 807\n",
            "good 806\n",
            "very 796\n",
            "make 794\n",
            "world 787\n",
            "still 782\n",
            "see 772\n",
            "own 772\n",
            "men 763\n",
            "work 762\n",
            "long 752\n",
            "here 750\n",
            "get 749\n",
            "both 730\n",
            "between 730\n",
            "life 715\n",
            "being 712\n",
            "under 707\n",
            "never 697\n",
            "day 687\n",
            "same 686\n",
            "another 684\n",
            "know 683\n",
            "while 680\n",
            "last 676\n",
            "us 675\n",
            "might 672\n",
            "great 665\n",
            "old 661\n",
            "year 658\n",
            "off 639\n",
            "come 630\n",
            "since 628\n",
            "against 627\n",
            "go 626\n",
            "came 622\n",
            "right 613\n",
            "used 611\n",
            "take 610\n",
            "three 610\n",
            "himself 603\n",
            "states 603\n",
            "few 601\n",
            "house 591\n",
            "use 591\n",
            "during 585\n",
            "without 583\n",
            "again 577\n",
            "place 570\n",
            "american 569\n",
            "around 562\n",
            "however 552\n",
            "home 547\n",
            "small 542\n",
            "found 536\n",
            "mrs. 534\n",
            "1 527\n",
            "thought 517\n",
            "went 507\n",
            "say 504\n",
            "part 500\n",
            "once 499\n",
            "general 498\n",
            "high 497\n",
            "upon 495\n",
            "school 493\n",
            "every 491\n",
            "don't 489\n",
            "does 485\n",
            "got 482\n",
            "united 482\n",
            "left 480\n",
            "number 472\n",
            "course 465\n",
            "war 464\n",
            "until 461\n",
            "always 458\n",
            "away 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count the number of sentences in Inaugural Address Corpus"
      ],
      "metadata": {
        "id": "8tnuhzGu-oHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inaugural.sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38efe6ec-3c88-4bd8-f180-7e6bd568c56b",
        "id": "vfXWcoTP-oHQ"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here each sentence is a sub array"
      ],
      "metadata": {
        "id": "6VQmSzEm-oHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sentences in inaugural corpus:\",len(inaugural.sents()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2346da-cbc0-498d-c11d-3ba5c4e0ed30",
        "id": "oE-Ttp7g-oHR"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in inaugural corpus: 5217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y45Kw0tY-oHR"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}